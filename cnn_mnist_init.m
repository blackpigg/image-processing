function net = cnn_mnist_init(varargin)
%==========================================================================
% CNN_MNIST_LENET Initialize a CNN similar for MNIST
opts.batchNormalization = true;
opts.networkType        = 'simplenn';
opts                    = vl_argparse(opts, varargin);

% control random number generation (shuffling method and its seed)
rng('default'); rng(0)

% 네트워크 정의
f = 1/100;
net.layers = {};
net.layers{end+1} = struct(...
    'type','conv',...
    'weights', {{f*randn(5, 5, 1, 20, 'single'), zeros(1, 20,'single')}},...
    'stride', 1, ...
    'pad',0);

net.layers{end+1} = struct(...
    'type','pool',...
    'method', 'max',...
    'pool',[2,2],...
    'stride', 2, ...
    'pad',0);

net.layers{end+1} = struct(...
    'type','conv',...
    'weights', {{f*randn(5, 5, 20, 50, 'single'), zeros(1, 50,'single')}},...
    'stride', 1, ...
    'pad',0);

net.layers{end+1} = struct(...
    'type','pool',...
    'method', 'max',...
    'pool',[2,2],...
    'stride', 2, ...
    'pad',0);

net.layers{end+1} = struct(...
    'type','conv',...
    'weights', {{f*randn(4, 4, 50, 500, 'single'), zeros(1, 500,'single')}},...
    'stride', 1, ...
    'pad',0);

net.layers{end+1} = struct(...
    'type', 'relu');

net.layers{end+1} = struct(...
    'type','conv',...
    'weights', {{f*randn(1, 1, 50, 10, 'single'), zeros(1, 10,'single')}},...
    'stride', 1, ...
    'pad',0);

net.layers{end+1} = struct(...
    'type', 'softmaxloss');

% TODO: Convolutional Layer
% TODO: Pooling Layer
% TODO: Convolutional Layer
% TODO: Pooling Layer
% TODO: Convolutional Layer
% TODO: ReLU Layer
% TODO: Convolutional Layer
% TODO: Softmax Loss Layer

% optionally switch to batch normalization
if opts.batchNormalization
    net = insertBnorm(net, 1); % 1, 3, 5번째 layer 뒤에 노멀라이제이션 레이어 삽입
    net = insertBnorm(net, 4);
    net = insertBnorm(net, 7);
end

% TODO: Meta parameters

net.meta.inputSize              = [28 28 1];
net.meta.trainOpts.learningRate = 0.001;
net.meta.trainOpts.numEpochs    = 20;
net.meta.trainOpts.batchSize    = 100;

% Fill in defaul values
net = vl_simplenn_tidy(net);

% network type 대응: DagNN로 구조 변경
switch lower(opts.networkType)
    case 'simplenn'
        % done
    case 'dagnn'
        net = dagnn.DagNN.fromSimpleNN(net, 'canonicalNames', true);
        net.addLayer(...
            'top1err', dagnn.Loss('loss', 'classerror'), ...
            {'prediction', 'label'}, 'error');
        net.addLayer(...
            'top5err', dagnn.Loss('loss', 'topkerror', ...
            'opts', {'topk', 5}), {'prediction', 'label'}, 'top5err');
    otherwise
        assert(false);
end
end